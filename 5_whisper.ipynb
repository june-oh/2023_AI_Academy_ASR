{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/june-oh/2023_AI_Academy_ASR/blob/main/5_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Whisper\n",
        "\n",
        "- https://openai.com/blog/whisper/  \n",
        "- trained on 680,000 hours of multilingual and multitask supervised data collected from the web.\n",
        "- multiple languages (https://github.com/openai/whisper/blob/main/whisper/tokenizer.py)\n",
        "\n",
        "<img src=\"https://cdn.openai.com/whisper/asr-summary-of-model-architecture-desktop.svg\">"
      ],
      "metadata": {
        "id": "0TURT2LUR6sF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper install\n",
        "https://github.com/openai/whisper"
      ],
      "metadata": {
        "id": "r1Q-AJcZj0Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone git repo"
      ],
      "metadata": {
        "id": "-AvhV4s0jwJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper model load"
      ],
      "metadata": {
        "id": "zRKZHWr1kR6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# whisper model load"
      ],
      "metadata": {
        "id": "xQXGgeRXkAQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# whisper modules"
      ],
      "metadata": {
        "id": "bG4n2FelvXSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `datasets` : audio, computer vision, nlp task 용 공유 데이터에 쉽게 접근할 수 있는 라이브러리, huggingface에서 사용됨"
      ],
      "metadata": {
        "id": "7Cr26TmapoRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install datasets"
      ],
      "metadata": {
        "id": "EWFMQZRKY2_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LibriSpeech dataset\n",
        "\n",
        "- approximately 1000 hours of 16kHz\n",
        "- read audiobooks from the LibriVox project\n",
        "\n",
        "https://www.openslr.org/12  \n",
        "https://paperswithcode.com/dataset/librispeech  \n",
        "https://huggingface.co/datasets/librispeech_asr"
      ],
      "metadata": {
        "id": "_UPl567wb4en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LibriSpeech dataset load"
      ],
      "metadata": {
        "id": "BefWgzbBbSMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LibriSpeech test data load\n",
        "english_ds = load_dataset(\"kresnik/librispeech_asr_test\", \"clean\")"
      ],
      "metadata": {
        "id": "lNLyVgdbc6N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check file list"
      ],
      "metadata": {
        "id": "cVzXsDvBeGN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check sample data"
      ],
      "metadata": {
        "id": "pYnkAR_ae-xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `IPython.display` : IPython 위젯을 사용할 수 있는 라이브러리"
      ],
      "metadata": {
        "id": "ZxfnfC8UoG0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd"
      ],
      "metadata": {
        "id": "moDoBvRYegk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# listen audio file using ipd.Audio"
      ],
      "metadata": {
        "id": "UZXCorOoe6r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## whisper model을 이용한 LibriSpeech 인식"
      ],
      "metadata": {
        "id": "JMUyfAKQkaOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transcribe using whisper model"
      ],
      "metadata": {
        "id": "UF45MyIzkC7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check result"
      ],
      "metadata": {
        "id": "T1fCle3ShiZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with reference"
      ],
      "metadata": {
        "id": "7eOLTB21hoeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `jiwer` : CER, WER 등 음성인식 결과 평가 관련 라이브러리"
      ],
      "metadata": {
        "id": "pk28o3CCoNzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install jiwer"
      ],
      "metadata": {
        "id": "Dvy3VQIuiIJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cer"
      ],
      "metadata": {
        "id": "XUpce7WCiNtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zeroth-Korean\n",
        "\n",
        "- 51.6시간 한국어 학습데이터 (22,263 발화, 105명, 3000 문장)  \n",
        "- 휴대폰으로 녹음\n",
        "- https://github.com/goodatlas/zeroth"
      ],
      "metadata": {
        "id": "j-_ACVt-i0s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zeroth-Korean dataset load\n",
        "korea_ds = load_dataset(\"kresnik/zeroth_korean\", \"clean\")"
      ],
      "metadata": {
        "id": "ffKsO0ZRizvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check file list"
      ],
      "metadata": {
        "id": "61r60hFemUsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check sample data"
      ],
      "metadata": {
        "id": "LGqvQ7z1maJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# listen audio file using ipd.Audio"
      ],
      "metadata": {
        "id": "QggATtXFme_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## whisper model을 이용한 Zeroth-Korean 인식"
      ],
      "metadata": {
        "id": "tu97U2jTpQlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transcribe using whisper model"
      ],
      "metadata": {
        "id": "3UfLxl4ZmnIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check result"
      ],
      "metadata": {
        "id": "VvAH0dS-mtby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with reference"
      ],
      "metadata": {
        "id": "wESVSw5WnM1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cer"
      ],
      "metadata": {
        "id": "PSpq33eRmxEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# re.sub('[-=+,#/\\?:^.@*\\\"※~ㆍ!』‘|\\(\\)\\[\\]`\\'…》\\”\\“\\’·]', '', text)"
      ],
      "metadata": {
        "id": "x0uNP8MmnkYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove special symbol"
      ],
      "metadata": {
        "id": "ahIrt9shm9ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cer"
      ],
      "metadata": {
        "id": "8PAiaE3qnCJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# whisper load_audio\n",
        "# whisper pad_or_trim"
      ],
      "metadata": {
        "id": "XB3BmG9Mpx0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# whisper log_mel_spectrogram"
      ],
      "metadata": {
        "id": "IkgHelL_p7Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `matplotlib` : 시각화용 라이브러리 "
      ],
      "metadata": {
        "id": "9F1qOw9Oqb2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mathplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pOR2jtOaqLm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(mel_cpu,aspect='auto',interpolation='nearest',origin='lower')"
      ],
      "metadata": {
        "id": "-xlIsZU0qBRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# whisper decode\n",
        "# options = whisper.DecodingOptions(fp16 = False)"
      ],
      "metadata": {
        "id": "FagjfVovqUDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실시간으로 녹음 후 Whisper Model로 인식 결과 확인\n",
        "Gradio를 이용해 간단한 Web UI를 구현해 본인의 목소리를 실시간으로 녹음하고  \n",
        "whisper model로 녹음한 음성 인식 결과 확인  \n",
        "https://github.com/innovatorved/whisper-openai-gradio-implementation"
      ],
      "metadata": {
        "id": "Gk4BQtREk39I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio"
      ],
      "metadata": {
        "id": "Xts_zXUboAff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr \n",
        "import time"
      ],
      "metadata": {
        "id": "q5qLebotkwSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SpeechToText(audio):\n",
        "    if audio == None : return \"\" \n",
        "    time.sleep(1)\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # Detect the Max probability of language\n",
        "    _, probs = model.detect_language(mel)\n",
        "    language = max(probs, key=probs.get)\n",
        "\n",
        "    #  Decode audio to Text\n",
        "    options = whisper.DecodingOptions(fp16 = False)\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    return (language , result.text)"
      ],
      "metadata": {
        "id": "zkndpeaYky0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    title = 'OpenAI Whisper implementation on Gradio Web UI', \n",
        "    fn=SpeechToText, \n",
        "    \n",
        "    inputs=[\n",
        "        gr.Audio(source=\"microphone\", type=\"filepath\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        \"label\",\n",
        "        \"textbox\",\n",
        "    ],\n",
        "    live=True\n",
        ").launch(\n",
        "    debug=False,\n",
        "    share=True\n",
        ")"
      ],
      "metadata": {
        "id": "16VPp_IDk0g0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}